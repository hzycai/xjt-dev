# main.py
import tkinter as tk
from tkinter import ttk, messagebox
import threading
import pyaudio
import numpy as np
import time
import os
import sys
import logging
import queue
from funasr import AutoModel
import pyvirtualcam

# å°è¯•å¯¼å…¥ faster-whisperï¼ˆæ‰“åŒ…åä¹Ÿèƒ½å·¥ä½œï¼‰
try:
    from faster_whisper import WhisperModel
    WHISPER_AVAILABLE = True
except Exception as e:
    WHISPER_AVAILABLE = False
    WHISPER_ERROR = str(e)

# æ£€æµ‹ GPU
try:
    import torch
    CUDA_AVAILABLE = torch.cuda.is_available()
except ImportError:
    CUDA_AVAILABLE = False

# æ·»åŠ OpenCVå¯¼å…¥ç”¨äºè§†é¢‘å¤„ç†
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False

# å¦‚æœtorchä¸å¯ç”¨ï¼Œå°è¯•onnxruntimeä½œä¸ºå¤‡é€‰

# è·å–èµ„æºè·¯å¾„ï¼ˆå…¼å®¹æ‰“åŒ…åå’Œå¼€å‘ç¯å¢ƒï¼‰
def resource_path(relative_path):
    """è·å– PyInstaller æ‰“åŒ…åçš„èµ„æºè·¯å¾„"""
    try:
        # PyInstaller åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤¹ _MEIPASS
        base_path = sys._MEIPASS
    except Exception:
        base_path = os.path.abspath(".")
    return os.path.join(base_path, relative_path)

class VoiceFilterApp:
    def __init__(self, root):
        self.root = root
        self.root.title("æŠ–éŸ³ç›´æ’­æ•æ„Ÿè¯è¿‡æ»¤å™¨")
        self.root.geometry("480x500")  # å¢åŠ çª—å£é«˜åº¦ä»¥å®¹çº³è§†é¢‘æ§ä»¶
        self.root.resizable(False, False)
        
        # Setup logging
        self.setup_logging()
        logging.info("Voice Filter App started")
        if CUDA_AVAILABLE:
            logging.info(f"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}")
        else:
            logging.info("CUDA is not available. Using CPU.")
        
        self.is_running = False
        self.caption_running = False
        self.whisper_running = False
        
       

        self.p = pyaudio.PyAudio()
        self.load_sensitive_words()
        self.model_sizes = self.discover_bundled_models()
        
        self.create_widgets()

    def setup_logging(self):
        """Setup logging configuration"""
        log_format = '%(asctime)s - %(levelname)s - %(message)s'
        logging.basicConfig(
            level=logging.INFO,
            format=log_format,
            handlers=[
                logging.FileHandler("log.txt", encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )

    def load_sensitive_words(self):
        """åŠ è½½å†…ç½®æ•æ„Ÿè¯åº“"""
        self.sensitive_set = set()
        try:
            # å°è¯•ä»configç›®å½•åŠ è½½æ•æ„Ÿè¯
            config_path = resource_path("config")
            if not os.path.exists(config_path):
                os.makedirs(config_path)
            sensitive_words_file = os.path.join(config_path, "sensitive_words.txt")
            
            # å°è¯•ä»å¤–éƒ¨æ–‡ä»¶åŠ è½½æ•æ„Ÿè¯
            with open(sensitive_words_file, "r", encoding="utf-8") as f:
                for line in f:
                    word = line.strip()
                    if word:  # å¿½ç•¥ç©ºè¡Œ
                        self.sensitive_set.add(word)
            logging.info(f"Loaded {len(self.sensitive_set)} sensitive words from file")
        except FileNotFoundError:
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æ•æ„Ÿè¯åº“
            logging.warning("sensitive_words.txt not found, using default sensitive words")
            self.sensitive_set = {"å¾®ä¿¡", "VX", "èµšé’±", "æœ€ä¾¿å®œ", "ç¨³èµš", " guaranteed", "ç»å¯¹", "ç¬¬ä¸€"}
        except Exception as e:
            logging.error(f"Error loading sensitive words: {e}")
            # å‡ºç°é”™è¯¯æ—¶ä½¿ç”¨é»˜è®¤æ•æ„Ÿè¯åº“
            self.sensitive_set = {"å¾®ä¿¡", "VX", "èµšé’±", "æœ€ä¾¿å®œ", "ç¨³èµš", " guaranteed", "ç»å¯¹", "ç¬¬ä¸€"}

    def discover_bundled_models(self):
        """è‡ªåŠ¨å‘ç°æ‰“åŒ…è¿›æ¥çš„æ¨¡å‹"""
        models_dir = resource_path("models")
        if not os.path.exists(models_dir):
            return ["base"]  # fallback
        
        available = []
        for model_name in ["base", "small", "medium"]:
            if os.path.isdir(os.path.join(models_dir, model_name)):
                available.append(model_name)
        return available if available else ["base"]

    def get_devices(self, kind='input'):
        devices = []
        for i in range(self.p.get_device_count()):
            dev = self.p.get_device_info_by_index(i)
            if kind == 'input' and dev['maxInputChannels'] > 0:
                devices.append((i, dev['name']))
            elif kind == 'output' and dev['maxOutputChannels'] > 0:
                devices.append((i, dev['name']))
        return devices

    # æ·»åŠ æ‘„åƒå¤´æ£€æµ‹æ–¹æ³•
    def get_camera_devices(self):
        """æ£€æµ‹å¯ç”¨çš„æ‘„åƒå¤´è®¾å¤‡"""
        if not CV2_AVAILABLE:
            return []
            
        cameras = []
        for i in range(10):  # æ£€æµ‹å‰10ä¸ªæ‘„åƒå¤´
            cap = cv2.VideoCapture(i)
            if cap.isOpened():
                ret, _ = cap.read()
                if ret:
                    cameras.append((i, f"æ‘„åƒå¤´ {i}"))
                cap.release()
        return cameras

    def create_widgets(self):
        # æ ‡é¢˜
        title = tk.Label(self.root, text="æŠ–éŸ³ç›´æ’­æ•æ„Ÿè¯è¿‡æ»¤å™¨", font=("Arial", 14, "bold"))
        title.pack(pady=(10, 5))

        # Whisper çŠ¶æ€
        if not WHISPER_AVAILABLE:
            status = tk.Label(self.root, text=f"âŒ Whisper åŠ è½½å¤±è´¥: {WHISPER_ERROR}", fg="red")
            status.pack(pady=(0,10))
            return

        device_info = "âœ… ä½¿ç”¨ GPU è¿›è¡Œè¯­éŸ³è¯†åˆ«" if CUDA_AVAILABLE else "âš ï¸ ä½¿ç”¨ CPU è¿›è¡Œè¯­éŸ³è¯†åˆ«"
        color = "green" if CUDA_AVAILABLE else "orange"
        tk.Label(self.root, text=device_info, fg=color).pack()

        # è¾“å…¥è®¾å¤‡
        tk.Label(self.root, text="è¾“å…¥è®¾å¤‡ï¼ˆéº¦å…‹é£ï¼‰:", anchor='w').pack(fill='x', padx=20, pady=(10,0))
        input_devices = self.get_devices('input')
        self.input_names = [name for _, name in input_devices]
        self.input_idx_map = {name: idx for idx, name in input_devices}
        self.input_combo = ttk.Combobox(self.root, values=self.input_names, state="readonly")
        if self.input_names:
            self.input_combo.current(0)
        self.input_combo.pack(fill='x', padx=20, pady=5)

        # è¾“å‡ºè®¾å¤‡ï¼ˆä¼˜å…ˆ VB-Cableï¼‰
        tk.Label(self.root, text="è¾“å‡ºè®¾å¤‡ï¼ˆæ¨è VB-Cableï¼‰:", anchor='w').pack(fill='x', padx=20, pady=(10,0))
        output_devices = self.get_devices('output')
        vb_devices = [name for _, name in output_devices if 'CABLE' in name.upper()]
        if not vb_devices:
            vb_devices = [name for _, name in output_devices]
        self.output_names = vb_devices
        self.output_idx_map = {name: idx for idx, name in output_devices}
        self.output_combo = ttk.Combobox(self.root, values=self.output_names, state="readonly")
        if self.output_names:
            self.output_combo.current(0)
        self.output_combo.pack(fill='x', padx=20, pady=5)

        # è§†é¢‘è¾“å…¥è®¾å¤‡é€‰æ‹©ï¼ˆæ–°å¢ï¼‰
        if CV2_AVAILABLE:
            tk.Label(self.root, text="è§†é¢‘è¾“å…¥è®¾å¤‡:", anchor='w').pack(fill='x', padx=20, pady=(10,0))
            camera_devices = self.get_camera_devices()
            self.camera_names = [name for _, name in camera_devices]
            self.camera_idx_map = {name: idx for idx, name in camera_devices}
            self.camera_combo = ttk.Combobox(self.root, values=self.camera_names, state="readonly")
            if self.camera_names:
                self.camera_combo.current(0)
            self.camera_combo.pack(fill='x', padx=20, pady=5)
            
            # è§†é¢‘è¾“å‡ºé€‰é¡¹ï¼ˆæ–°å¢ï¼‰
            self.video_output_var = tk.BooleanVar()
            video_output_check = tk.Checkbutton(self.root, text="å¯ç”¨è§†é¢‘è¾“å‡ºï¼ˆ2ç§’å»¶è¿Ÿï¼‰", variable=self.video_output_var)
            video_output_check.pack(pady=5)

        # æ¨¡å‹é€‰æ‹©
        tk.Label(self.root, text="Whisper æ¨¡å‹:", anchor='w').pack(fill='x', padx=20, pady=(10,0))
        self.model_var = tk.StringVar(value=self.model_sizes[0])
        model_combo = ttk.Combobox(self.root, textvariable=self.model_var, values=self.model_sizes, state="readonly")
        model_combo.pack(fill='x', padx=20, pady=5)
        model_tip = "â€¢ base: ä½å»¶è¿Ÿï¼Œé€‚åˆ CPU\nâ€¢ small: æ›´å‡†ç¡®ï¼Œæ¨è GPU"
        tk.Label(self.root, text=model_tip, fg="gray", justify='left').pack(anchor='w', padx=20)

        # å¯åŠ¨æŒ‰é’®
        btn_frame = tk.Frame(self.root)
        btn_frame.pack(pady=20)
        self.start_btn = tk.Button(btn_frame, text="â–¶ å¯åŠ¨è¿‡æ»¤", command=self.toggle_process, width=15, height=2)
        self.start_btn.pack()

        # çŠ¶æ€æ 
        self.status_frame = tk.Frame(self.root)
        self.status_frame.pack(pady=5)
        
        self.mic_status = tk.Label(self.status_frame, text="ğŸ¤ éº¦å…‹é£: æœªè¿æ¥", fg="gray")
        self.mic_status.pack(side=tk.LEFT, padx=5)
        
        self.filter_status = tk.Label(self.status_frame, text="ğŸ” è¿‡æ»¤: æœªè¿è¡Œ", fg="gray")
        self.filter_status.pack(side=tk.LEFT, padx=5)
        
        self.cable_status = tk.Label(self.status_frame, text="ğŸ”Œ Cable: æœªæ£€æµ‹", fg="gray")
        self.cable_status.pack(side=tk.LEFT, padx=5)
        
        # è§†é¢‘çŠ¶æ€ï¼ˆæ–°å¢ï¼‰
        if CV2_AVAILABLE:
            self.video_status = tk.Label(self.status_frame, text="ğŸ“¹ è§†é¢‘: æœªè¿è¡Œ", fg="gray")
            self.video_status.pack(side=tk.LEFT, padx=5)

        # åº•éƒ¨æç¤º
        hint = tk.Label(self.root, text="ä½¿ç”¨å‰è¯·å®‰è£… VB-Cable\nç›´æ’­ä¼´ä¾£ä¸­é€‰æ‹© 'CABLE Input' ä½œä¸ºéº¦å…‹é£", fg="gray")
        hint.pack(side='bottom', pady=(0,10))

    def toggle_process(self):
        if not self.is_running:
            self.start_process()
        else:
            self.stop_process()

    def start_process(self):
        try:
            input_name = self.input_combo.get()
            output_name = self.output_combo.get()
            if not input_name or not output_name:
                messagebox.showerror("é”™è¯¯", "è¯·é€‰æ‹©è¾“å…¥å’Œè¾“å‡ºè®¾å¤‡")
                logging.error("Input or output device not selected")
                return

            self.input_idx = self.input_idx_map[input_name]
            self.output_idx = self.output_idx_map[output_name]
            self.selected_model = self.model_var.get()
            
            # Output all selected device indices
            logging.info(f"Selected input device index: {self.input_idx}")
            logging.info(f"Selected output device index: {self.output_idx}")
            
            # If video is available and enabled, log camera device index
            if CV2_AVAILABLE and self.video_output_var.get() and self.camera_names:
                camera_name = self.camera_combo.get()
                if camera_name:
                    self.camera_idx = self.camera_idx_map[camera_name]
                    logging.info(f"Selected camera device index: {self.camera_idx}")
            
            logging.info(f"Starting process with input: {input_name}, output: {output_name}, model: {self.selected_model}")

            # æ›´æ–°Cableè¾“å‡ºçŠ¶æ€
            cable_detected = 'CABLE' in output_name.upper()
            self.cable_status.config(text=f"{'âœ…' if cable_detected else 'âŒ'} Cable: {'å·²æ£€æµ‹' if cable_detected else 'æœªæ£€æµ‹'}", 
                                     fg="green" if cable_detected else "red")

            self.is_running = True
            self.start_btn.config(text="â¹ åœæ­¢è¿‡æ»¤", state='disabled')
            
            # æ›´æ–°è¿‡æ»¤çŠ¶æ€
            self.filter_status.config(text="ğŸ” è¿‡æ»¤: åˆå§‹åŒ–...", fg="orange")
            
            self.process_thread = threading.Thread(target=self.run_filter, daemon=True)
            self.process_thread.start()
            
          
            # å¼‚æ­¥å¯ç”¨æŒ‰é’®ï¼ˆé¿å…å¡æ­»ï¼‰
            self.root.after(1000, lambda: self.start_btn.config(state='normal'))
            
        except Exception as e:
            messagebox.showerror("é”™è¯¯", f"å¯åŠ¨å¤±è´¥:\n{str(e)}")
            logging.error(f"Failed to start process: {str(e)}")
            self.is_running = False

    def output_audio_to_vb_cable(self, audio_data, vb_cable_stream, input_rate, output_rate):
        """
        å°†éŸ³é¢‘æ•°æ®è¾“å‡ºåˆ°VB-Cableè¾“å‡ºæµï¼Œå¹¶è¿›è¡Œé‡é‡‡æ ·
        
        Args:
            audio_data: éœ€è¦è¾“å‡ºçš„éŸ³é¢‘æ•°æ®
            vb_cable_stream: VB-Cableè¾“å‡ºæµå¯¹è±¡
            input_rate: è¾“å…¥éŸ³é¢‘çš„é‡‡æ ·ç‡
            output_rate: è¾“å‡ºéŸ³é¢‘çš„é‡‡æ ·ç‡
        """
        try:
            # å¦‚æœé‡‡æ ·ç‡ä¸åŒï¼Œåˆ™è¿›è¡Œé‡é‡‡æ ·
            if input_rate != output_rate:
                # å°†bytesè½¬æ¢ä¸ºnumpy array
                audio_array = np.frombuffer(audio_data, dtype=np.float32)
                
                # è®¡ç®—é‡é‡‡æ ·åçš„é•¿åº¦
                new_length = int(len(audio_array) * output_rate / input_rate)
                
                # é‡é‡‡æ · - ç®€å•çš„çº¿æ€§æ’å€¼æ–¹æ³•
                resampled_audio = np.interp(
                    np.linspace(0, len(audio_array), new_length, endpoint=False),
                    np.arange(len(audio_array)),
                    audio_array
                )
                
                # è½¬æ¢å›bytes
                audio_data = resampled_audio.astype(np.float32).tobytes()
            
            # è¾“å‡ºéŸ³é¢‘æ•°æ®
            vb_cable_stream.write(audio_data)
            
        except Exception as e:
            logging.error(f"è¾“å‡ºéŸ³é¢‘åˆ°è¾“å‡ºè®¾å¤‡å¤±è´¥: {e}")
            print(f"è¾“å‡ºéŸ³é¢‘åˆ°è¾“å‡ºè®¾å¤‡å¤±è´¥: {e}")

    # æ·»åŠ è§†é¢‘è¾“å‡ºæ–¹æ³•
    def run_video_output(self):
        """è¿è¡Œè§†é¢‘è¾“å‡ºåŠŸèƒ½ï¼Œå®ç°2ç§’å»¶è¿Ÿæ•ˆæœ"""
        if not CV2_AVAILABLE:
            return
            
         
       
        cam = None
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("Cannot open camera 0")
            exit()
        try:
            # ä½¿ç”¨è™šæ‹Ÿæ‘„åƒå¤´ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            with pyvirtualcam.Camera(width=1280, height=720, fps=30) as cam:
                print(f'Using virtual camera: {cam.device}')
                
                # æ‰“å¼€æ‘„åƒå¤´0
                
                
                # è®¾ç½®æ‘„åƒå¤´åˆ†è¾¨ç‡
                cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
                cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
                
                # è®¡ç®—2ç§’çš„å¸§æ•°
                fps = 30
                delay_frames =int(3 * fps)   # 2ç§’å»¶è¿Ÿ
                
                # åˆ›å»ºå¸§ç¼“å†²åŒº
                frame_buffer = []
                
                # è®°å½•å¼€å§‹æ—¶é—´ï¼Œç”¨äºæ‰‹åŠ¨æ§åˆ¶å¸§ç‡
                start_time = time.time()
                frame_duration = 1.0 / fps
                
                # æˆ–è€…åˆ›å»ºä¸€ä¸ªåŠ¨æ€ç”»é¢ï¼ˆä¾‹å¦‚æ¸å˜è‰²ï¼‰
                while self.is_running:
                    # ä»æ‘„åƒå¤´è¯»å–å¸§
                    ret, frame = cap.read()
                    if not ret:
                        print("Can't receive frame from camera 0")
                        break
                        
                    # è°ƒæ•´å¸§å¤§å°ä»¥åŒ¹é…è™šæ‹Ÿæ‘„åƒå¤´åˆ†è¾¨ç‡
                    frame = cv2.resize(frame, (cam.width, cam.height))
                    
                    # å°†å¸§æ·»åŠ åˆ°ç¼“å†²åŒº
                    frame_buffer.append(frame.copy())
                    
                    # å¦‚æœç¼“å†²åŒºè¶…è¿‡å»¶è¿Ÿå¸§æ•°ï¼Œåˆ™ç§»é™¤å¹¶å‘é€æœ€æ—§çš„å¸§
                    if len(frame_buffer) > delay_frames:
                        # ç§»é™¤å¹¶è·å–æœ€æ—§çš„å¸§
                        delayed_frame = frame_buffer.pop(0)
                        
                        # å°†BGRæ ¼å¼è½¬æ¢ä¸ºRGBæ ¼å¼ï¼ˆå› ä¸ºpyvirtualcamæœŸæœ›RGBæ ¼å¼ï¼‰
                        frame_rgb = cv2.cvtColor(delayed_frame, cv2.COLOR_BGR2RGB)
                        
                        # å°†RGBæ ¼å¼çš„numpyæ•°ç»„å‘é€åˆ°è™šæ‹Ÿæ‘„åƒå¤´
                        cam.send(frame_rgb)
                        
                        # æ§åˆ¶å¸§ç‡ï¼Œé¿å… _last_frame_t ä¸º None çš„é—®é¢˜
                        current_time = time.time()
                        elapsed_time = current_time - start_time
                        if elapsed_time < frame_duration:
                            time.sleep(frame_duration - elapsed_time)
                        start_time = time.time()
                        
                    # ç­‰å¾…ä¸‹ä¸€å¸§
                    # Only call sleep_until_next_frame if we have sent at least one frame
                    if len(frame_buffer) > delay_frames or len(frame_buffer) == delay_frames:
                        try:
                            cam.sleep_until_next_frame()
                        except TypeError:
                            # Fallback if sleep_until_next_frame fails due to _last_frame_t being None
                            time.sleep(frame_duration)
        except Exception as e:
            logging.error(f"è§†é¢‘è¾“å‡ºé”™è¯¯: {e}")
        finally:
      
            if cap:
                cap.release()
            # æ›´æ–°è§†é¢‘çŠ¶æ€
            self.root.after(0, lambda: self.video_status.config(text="ğŸ“¹ è§†é¢‘: æœªè¿è¡Œ", fg="gray"))
            logging.info("è§†é¢‘è¾“å‡ºå·²åœæ­¢")

    def run_filter(self):
        stream_in = None
        vb_cable_stream = None
        stream_out = None
        
        # Define threads as None initially
        capture_thread = None
        recognition_thread = None
        
        # Record system startup time
        system_startup_time = time.time()
        
        try:
            # === åŠ è½½å†…ç½® Whisper æ¨¡å‹ ===
            # Replace with simpler model initialization from test_microphone_with_model.py
            # model = AutoModel(model="paraformer-zh-streaming", model_revision="v2.0.4", disable_update=True)
            model = AutoModel(model="paraformer-zh-streaming", model_revision="v2.0.4", disable_update=True)


            # === éŸ³é¢‘å‚æ•° ===
            INPUT_RATE = 16000   # Whisper æœ€ä½³è¾“å…¥
            OUTPUT_RATE = 48000  # VB-Cable å…¼å®¹ç‡
            CHUNK = 960          # Changed from 1024 to 960 to match test file
            CHANNELS = 1

            # æ›´æ–°éº¦å…‹é£çŠ¶æ€ - moved here to update immediately when process starts
            self.root.after(0, lambda: self.mic_status.config(text="ğŸ¤ éº¦å…‹é£: å·²è¿æ¥", fg="green"))
            print("CHANNELS======>",CHANNELS)
            print("self.input_idx======>",self.input_idx)
            # === æ‰“å¼€éŸ³é¢‘æµ ===
            stream_in = self.p.open(
                format=pyaudio.paFloat32,
                channels=CHANNELS,
                rate=INPUT_RATE,
                input=True,
                input_device_index=self.input_idx,
                frames_per_buffer=CHUNK
            )

            # æ‰“å¼€åˆ°é€‰å®šè¾“å‡ºè®¾å¤‡çš„è¾“å‡ºæµï¼Œä½¿ç”¨è®¾å¤‡æ”¯æŒçš„é‡‡æ ·ç‡
            device_info = self.p.get_device_info_by_index(self.output_idx)
            output_rate = int(device_info['defaultSampleRate'])
            
            vb_cable_stream = self.p.open(
                format=pyaudio.paFloat32,
                channels=1,  # æ ¹æ®è¾“å…¥éŸ³é¢‘é€šé“æ•°è°ƒæ•´
                rate=output_rate,  # ä½¿ç”¨è¾“å‡ºè®¾å¤‡æ”¯æŒçš„é‡‡æ ·ç‡
                output=True,
                output_device_index=self.output_idx,
                frames_per_buffer=1024
            )
            
            logging.info(f"Audio streams opened - input rate: {INPUT_RATE}, output rate: {OUTPUT_RATE}")

            # æ›´æ–°è¿‡æ»¤çŠ¶æ€
            self.root.after(0, lambda: self.filter_status.config(text="âœ… è¿‡æ»¤: è¿è¡Œä¸­", fg="green"))

            # === ç¼“å†²ä¸çŠ¶æ€ ===
            audio_queue = queue.Queue()
            last_process_time = time.time()
            mute_intervals = []  # [(global_start_time, global_end_time), ...]
            
            # ä½¿ç”¨ tk.Label é€‰ä¸­çš„è¾“å‡ºè®¾å¤‡
            selected_output_device_idx = self.output_idx
            
            # Thread 1: Audio capture and queue management
            def capture_audio(vb_cable_stream):
                audio_buffer = np.array([], dtype=np.float32)
                while self.is_running:
                    try:
                        self.caption_running = True
                        # è¯»å–éŸ³é¢‘å—
                        data = stream_in.read(CHUNK, exception_on_overflow=False)
                        chunk_audio = np.frombuffer(data, dtype=np.float32)
                        audio_buffer = np.concatenate([audio_buffer, chunk_audio])
                        
                        # æ¯ 1.2 ç§’å¤„ç†ä¸€æ¬¡ (changed from 3 seconds)
                        if len(audio_buffer) >= int(1.2 * INPUT_RATE) and (time.time() - last_process_time) > 1.0:
                            # Put audio data into queue for processing
                            audio_queue.put(audio_buffer.copy())
                            # Log the buffer length
                            logging.info(f"Audio buffer length: {len(audio_buffer)} samples put into queue")
                            audio_buffer = np.array([], dtype=np.float32)
                            
                    except Exception as e:
                        logging.error(f"Audio capture error: {e}")
                        print(f"éŸ³é¢‘æ•è·é”™è¯¯: {e}")
                        break
                self.caption_running = False
                logging.info("is_running=false capture_audio threads finished")
                    
                        
            # Thread 2: Speech recognition
            def recognize_speech():
                buffer_start_time = time.time() - 3.0  # Changed from 1.2 to 3.0
                while True:  # Keep thread alive
                    try:
                        if self.is_running:
                            self.whisper_running = True
                            # Get audio data from queue
                            audio_data = audio_queue.get(timeout=0.1)

                            # Simplified recognition like in test_microphone_with_model.py
                            start_time = time.time()
                            res = model.generate(input=audio_data, is_final=self.is_running)
                            end_time = time.time()
                            recognition_time = end_time - start_time
                            logging.info(f"Recognized speech: {res}")
                            
                            # Output recognition time
                            print(f"Recognition result: {res}")
                            print(f"Recognition time: {recognition_time:.4f} seconds")
                            # if recognition_time < 0.5:
                            #     print("âš ï¸  Warning: Recognition time is less than 0.5 seconds.")
                            #     time.sleep(0.3 - recognition_time)
                            # æ£€æŸ¥æ•æ„Ÿè¯
                            found_sensitive_words = []
                            # è®°å½•æ‰¾åˆ°çš„æ•æ„Ÿè¯
                            # ä¿®æ”¹:æ£€æŸ¥è¯†åˆ«ç»“æœä¸­çš„æ•æ„Ÿè¯
                            if isinstance(res, list) and len(res) > 0 and 'text' in res[0]:
                                recognized_text = res[0]['text']
                                for word in self.sensitive_set:
                                    if word in recognized_text:
                                        found_sensitive_words.append(word)
                                        logging.warning(f"Sensitive word detected: {word}")
                                        print(f"âš ï¸  Warning: Sensitive word detected: {word}")
                                        
                            if found_sensitive_words:
                                logging.info(f"Found sensitive words: {', '.join(found_sensitive_words)}")
                                
                            # å¤„ç†éŸ³é¢‘è¾“å‡º - æ ¹æ®æ•æ„Ÿè¯æ£€æµ‹ç»“æœå†³å®šè¾“å‡ºåŸéŸ³é¢‘è¿˜æ˜¯å“”éŸ³
                            if selected_output_device_idx is not None:
                                # Calculate time interval from system startup
                                current_time = time.time()
                                time_interval = current_time - system_startup_time
                                print(f"Time since system startup: {time_interval:.4f} seconds")
                                
                                if found_sensitive_words:
                                    # åˆ›å»ºå“”éŸ³æ›¿ä»£éŸ³é¢‘
                                    beep_duration = len(audio_data) / INPUT_RATE  # è®¡ç®—éŸ³é¢‘æ—¶é•¿
                                    beep_freq = 800  # å“”éŸ³é¢‘ç‡ (Hz)
                                    
                                    # ç”Ÿæˆå“”éŸ³æ ·æœ¬æ•°
                                    num_samples = int(beep_duration * output_rate)
                                    # ç”Ÿæˆæ—¶é—´è½´
                                    t = np.linspace(0, beep_duration, num_samples, False)
                                    # ç”Ÿæˆå“”éŸ³ä¿¡å·
                                    beep_signal = np.sin(2 * np.pi * beep_freq * t) * 0.5  # é™ä½éŸ³é‡é¿å…åˆºè€³
                                    # è½¬æ¢ä¸ºfloat32æ ¼å¼
                                    beep_data = beep_signal.astype(np.float32).tobytes()
                                    
                                    # è¾“å‡ºå“”éŸ³
                                    self.output_audio_to_vb_cable(beep_data, vb_cable_stream, output_rate, output_rate)
                                else:
                                    # è¾“å‡ºåŸå§‹éŸ³é¢‘
                                    audio_bytes = audio_data.astype(np.float32).tobytes()
                                    self.output_audio_to_vb_cable(audio_bytes, vb_cable_stream, INPUT_RATE, output_rate)
                            
                            audio_queue.task_done()
                        else:
                            # When not running, clear the queue
                            while not audio_queue.empty():
                                try:
                                    audio_queue.get_nowait()
                                    audio_queue.task_done()
                                except queue.Empty:
                                    break
                            self.whisper_running = False
                            
                            # Small sleep to prevent busy waiting
                            
                    except queue.Empty:
                        continue
                    except Exception as e:
                        logging.error(f"Speech recognition error: {e}")
                        print(f"è¯­éŸ³è¯†åˆ«é”™è¯¯: {e}")

                        
            # Start both threads
            capture_thread = threading.Thread(target=capture_audio, args=(vb_cable_stream,), daemon=True)
            recognition_thread = threading.Thread(target=recognize_speech, daemon=True)
            
            capture_thread.start()
            recognition_thread.start()
            print(f"CV2_AVAILABLE:{CV2_AVAILABLE}")
            print(f"self.video_output_var.get():{self.video_output_var.get()}")
            print(f"self.camera_names:{self.camera_names}")

            # å¦‚æœå¯ç”¨äº†è§†é¢‘è¾“å‡ºï¼Œåˆ™å¯åŠ¨è§†é¢‘çº¿ç¨‹
            if CV2_AVAILABLE and self.video_output_var.get() and self.camera_names:
                print("è¿›å…¥è§†é¢‘çº¿ç¨‹")
                camera_name = self.camera_combo.get()
                if camera_name:
                    self.camera_idx = self.camera_idx_map[camera_name]
                    video_thread = threading.Thread(target=self.run_video_output, daemon=True)
                    video_thread.start()
                    print("è§†é¢‘çº¿ç¨‹å·²å¯åŠ¨")
                else:
                    print("æœªé€‰æ‹©è§†é¢‘è¾“å…¥è®¾å¤‡")
            
            # Wait for threads to finish
            # capture_thread.join()
            recognition_thread.join()

        except Exception as e:
            logging.error(f"Main process error: {e}")
            print(f"ä¸»æµç¨‹é”™è¯¯: {e}")
            self.root.after(0, lambda: messagebox.showerror("é”™è¯¯", f"å¤„ç†å¼‚å¸¸:\n{str(e)}"))

        finally:
            # æ¸…ç†æ‰€æœ‰æ‰“å¼€çš„éŸ³é¢‘æµ
            if stream_in is not None:
                try:
                    stream_in.stop_stream()
                    stream_in.close()
                    logging.info("Input audio stream closed")
                except Exception as e:
                    logging.error(f"Error closing input stream: {e}")
                    
            if vb_cable_stream is not None:
                try:
                    vb_cable_stream.stop_stream()
                    vb_cable_stream.close()
                    logging.info("VB-Cable audio stream closed")
                except Exception as e:
                    logging.error(f"Error closing VB-Cable stream: {e}")
                    
            self.is_running = False
            self.root.after(0, lambda: self.mic_status.config(text="ğŸ¤ éº¦å…‹é£: æœªè¿æ¥", fg="gray"))
            self.root.after(0, lambda: self.filter_status.config(text="ğŸ” è¿‡æ»¤: æœªè¿è¡Œ", fg="gray"))
            # self.root.after(0, lambda: self.start_btn.config(text="â–¶ å¯åŠ¨è¿‡æ»¤", state='normal'))
            logging.info("Processing stopped")

    def find_vb_cable_device(self):
        """
        æŸ¥æ‰¾ç³»ç»Ÿä¸­çš„ VB-Cable è®¾å¤‡
        è¿”å›è®¾å¤‡ç´¢å¼•ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
        """
        try:
            devices = []
            for i in range(self.p.get_device_count()):
                dev = self.p.get_device_info_by_index(i)
                # æŸ¥æ‰¾è¾“å‡ºè®¾å¤‡ä¸”åç§°åŒ…å« CABLE
                if dev['maxOutputChannels'] > 0 and 'CABLE' in dev['name'].upper():
                    logging.info(f"Found VB-Cable device {i}: {dev['name']}")
                    return i
            logging.info("No VB-Cable device found")
            return None
        except Exception as e:
            logging.error(f"Error finding VB-Cable device: {e}")
            return None

    def stop_process(self):
        self.is_running = False
     
        logging.info("Stop process requested")
        
        # Update button to show stopping state
        self.start_btn.config(state='disabled', text="â¹ åœæ­¢ä¸­...", fg="gray")
        
        while (self.whisper_running or self.caption_running) :
            logging.info("Waiting for threads to finish...")
            self.root.update()  # Keep UI responsive
            time.sleep(0.1)
            
        # Reset button to initial state
        self.start_btn.config(text="â–¶ å¯åŠ¨è¿‡æ»¤", state='normal', fg="black")
        logging.info("Process stopped")

    def on_closing(self):
        if self.is_running:
            self.stop_process()
            # ç­‰å¾…çº¿ç¨‹ç»“æŸï¼ˆæœ€å¤š2ç§’ï¼‰
            if self.process_thread and self.process_thread.is_alive():
                self.process_thread.join(timeout=2.0)
          
        self.p.terminate()
        self.root.destroy()
        logging.info("Application closed")

if __name__ == "__main__":
    root = tk.Tk()
    app = VoiceFilterApp(root)
    root.protocol("WM_DELETE_WINDOW", app.on_closing)
    root.mainloop()